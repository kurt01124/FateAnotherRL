name: fateanother

# FateAnother RL v4 — Scaled Architecture (15 Inference Shards, 25x speed)
#
# 1 Trainer (Python/GPU)  → 모델 학습, per-hero .pt 갱신
# 15 Inference (C++/GPU)  → 각각 WC3 1개 담당 (25x), model hot-reload
# 15 WC3 (1 per inference) → 게임 실행, STATE 전송
# 1 TensorBoard           → 학습 모니터링
#
# 25x 속도: WC3 1개 : 추론 1개 (1:1 비율)
# 모든 추론 서버가 공유 볼륨의 per-hero .pt를 hot-reload
#
# Usage:
#   docker compose -f docker/docker-compose.v4.yml up -d
#   docker compose -f docker/docker-compose.v4.yml logs -f trainer
#   docker compose -f docker/docker-compose.v4.yml logs -f inference-1
#   docker compose -f docker/docker-compose.v4.yml down

# ── Reusable Fragments ──

x-gpu-deploy: &gpu-deploy
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]

x-inference: &inference
  image: fateanother-gpu:latest
  entrypoint: ["/entrypoint.inference.sh"]
  environment:
    INFERENCE_PORT: "7777"
    ACTION_PORT: "7778"
    DEVICE: "cuda"
    MODEL_DIR: "/data/models"
    ROLLOUT_DIR: "/data/rollouts"
    ROLLOUT_SIZE: "${ROLLOUT_SIZE:-2048}"
  volumes:
    - data:/data
  depends_on:
    - trainer
  deploy:
    <<: *gpu-deploy
  restart: unless-stopped
  shm_size: "128m"

x-wc3-env: &wc3-env
  WC3_SPEED_MULTIPLIER: "${WC3_SPEED:-25}"
  INFERENCE_PORT: "7777"
  RL_RECV_PORT: "7778"
  WINEDEBUG: "-all"
  VIS_HERO_ONLY: "${VIS_HERO_ONLY:-0}"

x-wc3-deploy: &wc3-deploy
  replicas: 1
  resources:
    limits:
      memory: 5G
      cpus: "2"

# ── Services ──

services:
  # ── Python Trainer (GPU, 학습만) ──
  trainer:
    image: fateanother-gpu:latest
    container_name: fate-trainer
    entrypoint: ["/entrypoint.trainer.sh"]
    environment:
      MODEL_DIR: "/data/models"
      ROLLOUT_DIR: "/data/rollouts"
      CHECKPOINT_DIR: "/data/checkpoints"
      LOG_DIR: "/data/runs"
      FRESH_START: "${FRESH_START:-0}"
      RESUME: "${RESUME:-auto}"
    volumes:
      - data:/data
    deploy:
      <<: *gpu-deploy
    networks:
      rlnet:
        ipv4_address: 172.30.0.10
    restart: unless-stopped
    shm_size: "256m"

  # ── Inference Servers (C++ libtorch, GPU × 15) ──
  inference-1:
    <<: *inference
    container_name: fate-inf-1
    networks:
      rlnet:
        ipv4_address: 172.30.0.20

  inference-2:
    <<: *inference
    container_name: fate-inf-2
    networks:
      rlnet:
        ipv4_address: 172.30.0.21

  inference-3:
    <<: *inference
    container_name: fate-inf-3
    networks:
      rlnet:
        ipv4_address: 172.30.0.22

  inference-4:
    <<: *inference
    container_name: fate-inf-4
    networks:
      rlnet:
        ipv4_address: 172.30.0.23

  inference-5:
    <<: *inference
    container_name: fate-inf-5
    networks:
      rlnet:
        ipv4_address: 172.30.0.24

  inference-6:
    <<: *inference
    container_name: fate-inf-6
    networks:
      rlnet:
        ipv4_address: 172.30.0.25

  inference-7:
    <<: *inference
    container_name: fate-inf-7
    networks:
      rlnet:
        ipv4_address: 172.30.0.26

  inference-8:
    <<: *inference
    container_name: fate-inf-8
    networks:
      rlnet:
        ipv4_address: 172.30.0.27

  inference-9:
    <<: *inference
    container_name: fate-inf-9
    networks:
      rlnet:
        ipv4_address: 172.30.0.28

  inference-10:
    <<: *inference
    container_name: fate-inf-10
    networks:
      rlnet:
        ipv4_address: 172.30.0.29

  inference-11:
    <<: *inference
    container_name: fate-inf-11
    networks:
      rlnet:
        ipv4_address: 172.30.0.30

  inference-12:
    <<: *inference
    container_name: fate-inf-12
    networks:
      rlnet:
        ipv4_address: 172.30.0.31

  inference-13:
    <<: *inference
    container_name: fate-inf-13
    networks:
      rlnet:
        ipv4_address: 172.30.0.32

  inference-14:
    <<: *inference
    container_name: fate-inf-14
    networks:
      rlnet:
        ipv4_address: 172.30.0.33

  inference-15:
    <<: *inference
    container_name: fate-inf-15
    networks:
      rlnet:
        ipv4_address: 172.30.0.34

  # ── WC3 Groups (1 replica × 15 groups = 15 instances) ──
  wc3-g1:
    image: fateanother-wc3:latest
    depends_on:
      - inference-1
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.20"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g2:
    image: fateanother-wc3:latest
    depends_on:
      - inference-2
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.21"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g3:
    image: fateanother-wc3:latest
    depends_on:
      - inference-3
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.22"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g4:
    image: fateanother-wc3:latest
    depends_on:
      - inference-4
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.23"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g5:
    image: fateanother-wc3:latest
    depends_on:
      - inference-5
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.24"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g6:
    image: fateanother-wc3:latest
    depends_on:
      - inference-6
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.25"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g7:
    image: fateanother-wc3:latest
    depends_on:
      - inference-7
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.26"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g8:
    image: fateanother-wc3:latest
    depends_on:
      - inference-8
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.27"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g9:
    image: fateanother-wc3:latest
    depends_on:
      - inference-9
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.28"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g10:
    image: fateanother-wc3:latest
    depends_on:
      - inference-10
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.29"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g11:
    image: fateanother-wc3:latest
    depends_on:
      - inference-11
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.30"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g12:
    image: fateanother-wc3:latest
    depends_on:
      - inference-12
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.31"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g13:
    image: fateanother-wc3:latest
    depends_on:
      - inference-13
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.32"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g14:
    image: fateanother-wc3:latest
    depends_on:
      - inference-14
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.33"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  wc3-g15:
    image: fateanother-wc3:latest
    depends_on:
      - inference-15
    environment:
      <<: *wc3-env
      INFERENCE_HOST: "172.30.0.34"
    networks:
      - rlnet
    restart: unless-stopped
    deploy:
      <<: *wc3-deploy

  # ── TensorBoard ──
  tensorboard:
    image: tensorflow/tensorflow:latest
    command: tensorboard --logdir /data/runs --host 0.0.0.0 --port 6006
    ports:
      - "6006:6006"
    volumes:
      - data:/data:ro
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    networks:
      - rlnet
    restart: unless-stopped

networks:
  rlnet:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/24

volumes:
  data:
